{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVmB6eqZAQ3D"
      },
      "source": [
        "**DOWNLOAD & LOAD THE DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyx5nmC95Y0J"
      },
      "source": [
        "RUN ON CONSOLE\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "  console.log(\"Working\"); \n",
        "  document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSxW9TvZAJwX"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kL-uNO1ASlQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQeBIzvIAVUU"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/ \n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9TnjGxwAVXV"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d wwwwwee/dehaze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGV78Ss0AVaW"
      },
      "outputs": [],
      "source": [
        "! unzip dehaze.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dFuimM3-vW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erYJItT84GQm"
      },
      "source": [
        "Create new directories for the training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_ltxHEEAVk5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content'\n",
        "train_hazy_dir = os.path.join(base_dir, 'train/hazy')\n",
        "train_clear_dir = os.path.join(base_dir, 'train/clear')\n",
        "val_hazy_dir = os.path.join(base_dir, 'val/hazy')\n",
        "val_clear_dir = os.path.join(base_dir, 'val/clear')\n",
        "\n",
        "os.makedirs(train_hazy_dir, exist_ok=True)\n",
        "os.makedirs(train_clear_dir, exist_ok=True)\n",
        "os.makedirs(val_hazy_dir, exist_ok=True)\n",
        "os.makedirs(val_clear_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LTE2X_G4Io9"
      },
      "source": [
        "Split the original dataset into training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZFrantaAVoO"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "hazy_dir = os.path.join(base_dir, 'haze')\n",
        "clear_dir = os.path.join(base_dir, 'clear_images')\n",
        "hazy_files = sorted(os.listdir(hazy_dir))\n",
        "clear_files = sorted(os.listdir(clear_dir))\n",
        "\n",
        "clear_train_files, clear_val_files = train_test_split(clear_files, test_size=0.2, random_state=42)\n",
        "\n",
        "for clear_file in clear_files:\n",
        "    clear_id = os.path.splitext(clear_file)[0]\n",
        "    matched_hazy_files = [hazy_file for hazy_file in hazy_files if hazy_file.startswith(clear_id + '_')]\n",
        "    \n",
        "    if clear_file in clear_train_files:\n",
        "        shutil.copy(os.path.join(clear_dir, clear_file), os.path.join(train_clear_dir, clear_file))\n",
        "        for matched_hazy_file in matched_hazy_files:\n",
        "            shutil.copy(os.path.join(hazy_dir, matched_hazy_file), os.path.join(train_hazy_dir, matched_hazy_file))\n",
        "    else:\n",
        "        shutil.copy(os.path.join(clear_dir, clear_file), os.path.join(val_clear_dir, clear_file))\n",
        "        for matched_hazy_file in matched_hazy_files:\n",
        "            shutil.copy(os.path.join(hazy_dir, matched_hazy_file), os.path.join(val_hazy_dir, matched_hazy_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8dCL172AVrh"
      },
      "outputs": [],
      "source": [
        "def map_data_generator(hazy_dir, clear_dir, batch_size, img_size, shuffle=True):\n",
        "    hazy_files = sorted(os.listdir(hazy_dir))\n",
        "    clear_files = sorted(os.listdir(clear_dir))\n",
        "\n",
        "    file_mapping = []\n",
        "    for clear_file in clear_files:\n",
        "        clear_id = os.path.splitext(clear_file)[0]\n",
        "        matched_hazy_files = [hazy_file for hazy_file in hazy_files if hazy_file.startswith(clear_id + '_')]\n",
        "        for matched_hazy_file in matched_hazy_files:\n",
        "            file_mapping.append((matched_hazy_file, clear_file))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(file_mapping)\n",
        "\n",
        "    while True:\n",
        "        for i in range(0, len(file_mapping), batch_size):\n",
        "            batch_files = file_mapping[i:i+batch_size]\n",
        "            batch_hazy_imgs = []\n",
        "            batch_clear_imgs = []\n",
        "\n",
        "            for hazy_file, clear_file in batch_files:\n",
        "                hazy_img = Image.open(os.path.join(hazy_dir, hazy_file)).resize(img_size)\n",
        "                clear_img = Image.open(os.path.join(clear_dir, clear_file)).resize(img_size)\n",
        "\n",
        "                hazy_img = np.array(hazy_img) / 127.5 - 1.0\n",
        "                clear_img = np.array(clear_img) / 127.5 - 1.0\n",
        "\n",
        "                batch_hazy_imgs.append(hazy_img)\n",
        "                batch_clear_imgs.append(clear_img)\n",
        "\n",
        "            yield np.array(batch_hazy_imgs), np.array(batch_clear_imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAnxRusI4RWv"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "img_size = (256, 256)\n",
        "\n",
        "train_gen = map_data_generator(train_hazy_dir, train_clear_dir, batch_size, img_size)\n",
        "val_gen = map_data_generator(val_hazy_dir, val_clear_dir, batch_size, img_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQNvVPXR4Tgt"
      },
      "source": [
        "Generator and Discriminator architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hea4s14_4Vzb"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    inputs = tf.keras.layers.Input(shape=(256, 256, 3))\n",
        "    k_init = tf.keras.initializers.glorot_uniform()\n",
        "    b_init = tf.keras.initializers.zeros()\n",
        "    regularizer = tf.keras.regularizers.l2(0.01)\n",
        "\n",
        "                                    ##<<-- Encoding Layers -->>##\n",
        "    conv = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                  bias_initializer = b_init, kernel_regularizer = regularizer)(inputs)\n",
        "    conv = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n",
        "\n",
        "    conv_up = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n",
        "    conv_up = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n",
        "                                    \n",
        "                                    ##<<-- Residual Layers -->>##\n",
        "    conv1_1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n",
        "    conv1_2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_1)\n",
        "    conv1_3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_2)\n",
        "    conc1 = tf.add(conv1_3, conv1_1)\n",
        "    conv1 = tf.keras.activations.relu(conc1)\n",
        "\n",
        "    conv2_1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1)\n",
        "    conv2_2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_1)\n",
        "    conv2_3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_2)\n",
        "    conc2 = tf.add(conv2_3, conv2_1)\n",
        "    conv2 = tf.keras.activations.relu(conc2)\n",
        "\n",
        "    conv3_1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2)\n",
        "    conv3_2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_1)\n",
        "    conv3_3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_2)\n",
        "    conv3_4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_3)\n",
        "    conv3_5 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_4)\n",
        "    conc3 = tf.add(conv3_5, conv3_1)\n",
        "    conv3 = tf.keras.activations.relu(conc3)\n",
        "\n",
        "    conv4_1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3)\n",
        "    conv4_2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_1)\n",
        "    conv4_3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_2)\n",
        "    conv4_4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_3)\n",
        "    conv4_5 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_4)\n",
        "    conc4 = tf.add(conv4_5, conv4_1)\n",
        "    conv4 = tf.keras.activations.relu(conc4)\n",
        "\n",
        "                                            ##<<-- Decoding Layers -->>##\n",
        "    deconv = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                             kernel_regularizer = regularizer)(conv4)\n",
        "    deconv = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                             kernel_regularizer = regularizer)(deconv)\n",
        "\n",
        "    conv = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n",
        "                  bias_initializer = b_init, kernel_regularizer = regularizer)(deconv)\n",
        "    conv = tf.keras.layers.Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
        "                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n",
        "    conc = tf.add(conv, inputs)\n",
        "    \n",
        "    output = tf.keras.activations.relu(conc)\n",
        "\n",
        "    outputs = tf.keras.layers.experimental.preprocessing.Resizing(256, 256, interpolation='bilinear')(output)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "def build_discriminator():\n",
        "    input_hazy = tf.keras.layers.Input(shape=(256, 256, 3))\n",
        "    input_clear = tf.keras.layers.Input(shape=(256, 256, 3))\n",
        "\n",
        "    x = tf.keras.layers.concatenate([input_hazy, input_clear])\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(512, 4, strides=1, padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[input_hazy, input_clear], outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEU4sWii4Y1b"
      },
      "source": [
        "Create the generator and discriminator models and Define the GAN model, loss functions, and optimizers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DneCYVyF4cBk"
      },
      "outputs": [],
      "source": [
        "def build_combined_model(generator, discriminator):\n",
        "    hazy_input = tf.keras.layers.Input(shape=(256, 256, 3))\n",
        "    generated_image = generator(hazy_input)\n",
        "    \n",
        "    resized_hazy_input = tf.keras.layers.experimental.preprocessing.Resizing(256, 256, interpolation='bilinear')(hazy_input)\n",
        "    resized_generated_image = tf.keras.layers.experimental.preprocessing.Resizing(256, 256, interpolation='bilinear')(generated_image)\n",
        "    \n",
        "    discriminator.trainable = False\n",
        "    discriminator_output = discriminator([resized_hazy_input, resized_generated_image])\n",
        "\n",
        "    return tf.keras.Model(inputs=hazy_input, outputs=[generated_image, discriminator_output])\n",
        "\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "l1_loss_object = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "combined_model = build_combined_model(generator, discriminator)\n",
        "combined_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                       loss=[l1_loss_object, loss_object],\n",
        "                       loss_weights=[100, 1])\n",
        "\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                      loss=loss_object)\n",
        "\n",
        "combined_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K33G_wi04e01"
      },
      "source": [
        "Implement the training loop. The training loop should include steps for training the discriminator and the generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1x5_M1-4kVN",
        "outputId": "43e92765-13e0-45d7-e34b-792c34784a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def train_gan(train_gen, val_gen, num_epochs, steps_per_epoch, checkpoint_frequency=5):\n",
        "    checkpoint_dir = './checkpoints'\n",
        "    drive_dir = '/content/drive/My Drive/DehazeModels/TrainedOutdoor'\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    \n",
        "    graph_disc_loss = []\n",
        "    graph_gen_l1_loss = []\n",
        "    graph_gen_adv_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        epoch_disc_loss = []\n",
        "        epoch_gen_l1_loss = []\n",
        "        epoch_gen_adv_loss = []\n",
        "\n",
        "        for step, (hazy_imgs, clear_imgs) in enumerate(train_gen):\n",
        "            batch_size = hazy_imgs.shape[0]\n",
        "\n",
        "            # Train the discriminator\n",
        "            generated_imgs = generator.predict(hazy_imgs)\n",
        "            real_labels = np.ones((batch_size, 32, 32, 1))\n",
        "            fake_labels = np.zeros((batch_size, 32, 32, 1))\n",
        "\n",
        "            real_loss = discriminator.train_on_batch([hazy_imgs, clear_imgs], real_labels)\n",
        "            fake_loss = discriminator.train_on_batch([hazy_imgs, generated_imgs], fake_labels)\n",
        "            disc_loss = 0.5 * (real_loss + fake_loss)\n",
        "            epoch_disc_loss.append(disc_loss)\n",
        "\n",
        "            # Train the generator (using the combined model)\n",
        "            valid_labels = np.ones((batch_size, 32, 32, 1))\n",
        "            combined_loss = combined_model.train_on_batch(hazy_imgs, [clear_imgs, valid_labels])\n",
        "            epoch_gen_l1_loss.append(combined_loss[0])\n",
        "            epoch_gen_adv_loss.append(combined_loss[1])\n",
        "\n",
        "            if step >= steps_per_epoch - 1:\n",
        "                break\n",
        "\n",
        "        # Print epoch progress and any other metrics you'd like to track\n",
        "        avg_disc_loss = np.mean(epoch_disc_loss)\n",
        "        avg_gen_l1_loss = np.mean(epoch_gen_l1_loss)\n",
        "        avg_gen_adv_loss = np.mean(epoch_gen_adv_loss)\n",
        "        print(f\"Discriminator loss: {avg_disc_loss:.4f}, Generator L1 loss: {avg_gen_l1_loss:.4f}, Generator adversarial loss: {avg_gen_adv_loss:.4f}\")\n",
        "\n",
        "        graph_disc_loss.append(avg_disc_loss)\n",
        "        graph_gen_l1_loss.append(avg_gen_l1_loss)\n",
        "        graph_gen_adv_loss.append(avg_gen_adv_loss)\n",
        "\n",
        "        print(\"graph_disc_loss [] - \", graph_disc_loss)\n",
        "        print(\"graph_gen_l1_loss [] - \", graph_gen_l1_loss)\n",
        "        print(\"graph_gen_adv_loss [] - \", graph_gen_adv_loss)\n",
        "\n",
        "        if (epoch + 1) % checkpoint_frequency == 0:\n",
        "\n",
        "        # Plot the curves\n",
        "            plt.plot(graph_disc_loss, 'r', label='Discriminator Loss')\n",
        "            plt.plot(graph_gen_l1_loss, 'g', label='Generator L1 Loss')\n",
        "            plt.plot(graph_gen_adv_loss, 'b', label='Generator Adversarial Loss')\n",
        "            plt.legend()\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title(f'GAN Loss Curves (Epoch {epoch + 1})')\n",
        "            plt.savefig(f'./loss_curves_epoch_{epoch + 1}.png')\n",
        "            plt.show()\n",
        "\n",
        "            # Save model weights at checkpoints\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'generator_{epoch + 1:04d}.h5')\n",
        "            generator.save_weights(checkpoint_path)\n",
        "            \n",
        "            # Save model to drive\n",
        "            drive_path = os.path.join(drive_dir, f'generator_{epoch + 1:04d}.h5')\n",
        "            generator.save_weights(drive_path)\n",
        "            \n",
        "\n",
        "            print(f\"Saved generator weights at {checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTP1W_P4mYz"
      },
      "source": [
        "Train the GAN using the generators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohPpb09U4o0I"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "num_epochs = 100\n",
        "steps_per_epoch = math.ceil(10000 / 32)\n",
        "train_gan(train_gen, val_gen, num_epochs, steps_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbsm1taDRc8"
      },
      "source": [
        "**EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QKoxWYJ1nwY",
        "outputId": "3a3d1faf-1f45-49e9-e259-c92316b2af3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# weights_path = '/content/drive/My Drive/DehazeModels/TrainedOutdoor/generator_0005.h5'\n",
        "weights_path = '/content/checkpoints/generator_0050.h5'\n",
        "generator.load_weights(weights_path)\n",
        "\n",
        "# Define path to test image\n",
        "image_path = '/content/drive/MyDrive/Test_images/0468_1_0.2.jpg'\n",
        "# image_path = '/content/drive/MyDrive/Test_Images/0469_0.95_0.2.jpg'\n",
        "# image_path = '/content/drive/MyDrive/Test_Images/0471_0.95_0.2.jpg'\n",
        "# image_path = '/content/drive/MyDrive/Test_Images/1000_10_0.74905.png'\n",
        "# image_path = '/content/drive/MyDrive/Test_images/1000_1_0.99837.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TxMfYhVFXOy",
        "outputId": "ed8eb169-0261-4730-ca57-518fd3221e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 473ms/step\n",
            "PSNR: 15.933494673525777\n",
            "SSIM: 0.7776261632381138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d00a0b5d9b06>:37: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim = structural_similarity(clear_image, dehazed_image, multichannel=True)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "# Load a hazy image\n",
        "input_image_path = image_path\n",
        "hazy_image = Image.open(input_image_path)\n",
        "\n",
        "# Preprocess the hazy image\n",
        "hazy_image = hazy_image.resize((256, 256))  # Resize the image to match the generator input size\n",
        "hazy_image = np.array(hazy_image) / 127.5 - 1.0  # Scale pixel values to the range [-1, 1]\n",
        "hazy_image = np.expand_dims(hazy_image, axis=0)  # Add a batch dimension\n",
        "\n",
        "# Pass the preprocessed image through the generator <MODEL>\n",
        "dehazed_image = generator.predict(hazy_image)\n",
        "\n",
        "# Post-process the dehazed image\n",
        "dehazed_image = np.squeeze(dehazed_image)  # Remove the batch dimension\n",
        "dehazed_image = (dehazed_image + 1.0) * 127.5  # Rescale pixel values to the range [0, 255]\n",
        "dehazed_image = np.clip(dehazed_image, 0, 255).astype(np.uint8)  # Clip values and convert to uint8\n",
        "\n",
        "# Display or save the dehazed image\n",
        "output_image_path = '/content/output-out.jpg'\n",
        "# output_image_path = '/content/output-indoor.png'\n",
        "Image.fromarray(dehazed_image).save(output_image_path)\n",
        "\n",
        "# Load the hazy and dehazed images as arrays\n",
        "hazy_image = np.array(Image.open(input_image_path).resize((256, 256)))\n",
        "dehazed_image = np.array(Image.open(output_image_path))\n",
        "\n",
        "clear_img_path = '/content/clear_images/0468.jpg'\n",
        "clear_image = np.array(Image.open(clear_img_path).resize((256, 256)))\n",
        "\n",
        "# Calculate the PSNR and SSIM scores\n",
        "psnr = peak_signal_noise_ratio(clear_image, dehazed_image)\n",
        "ssim = structural_similarity(clear_image, dehazed_image, multichannel=True)\n",
        "\n",
        "# Print the scores\n",
        "print('PSNR:', psnr)\n",
        "print('SSIM:', ssim)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
